\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage{framed}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{graphicx}
\setlength{\columnsep}{1in}
\begin{document}

\newcommand{\Name}[1]{\noindent \textbf{Name:} #1 \\}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\psderiv}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}

\begin{center}
    \bf
    Machine Learning \\
    Computer Science 158 \\
    Spring 2017 \\
    \rm
    Problem Set 6\\
    Due:  March 5 at 11:59 PM \\
\end{center}
\noindent \textbf{Name: Madison Hobbs} \\

%-------------------- Problem 1 -----------------------%

\section{Feature Extraction [2 pts]}

\section{Digit Recognition using Bagging [5+3 pts]}

In this problem, we will study bagging and random forest on a handwritten digit dataset. This dataset contains 1797 samples (each having 64 features) of hand-written digits, classified into 10 classes.4 However, for this problem, we will only use 720 examples from 4 classes.

\begin{enumerate}[(a)]
	\item \textbf{(3 pts)} Run \verb+main(...)+ to compare the performance of these ensemble classifiers using 100 random splits of the digits dataset into training and test sets, where the test set contains roughly 20\% of the data. This generates two plots : 
We vary the number of features considered for the random forest from 1 to 65 (in step sizes of 2). Then, on a single plot, we plot the accuracy of the bagging classifier against that of the random forest with varying number of features. (Note: This code may take several minutes to run.)

\begin{center}
\includegraphics[width = 15cm, height = 10cm]{bag_vs_rf.png}
\end{center}

\begin{center}
\includegraphics[width = 15cm, height = 10cm]{accuracy.png}
\end{center}
Include these plots in your writeup. How does performance vary with the number of features considered per split? Which classifier performs better, and why?

\textbf{Solution: \\ \\ Random Forest performance varies with the number of features considered at each split, while the bagged trees remains constant. The Random Forest consistently performed better (higher accuracy) than bagged trees. This is probably because the Random Forest has decorrelated trees (each trained using a different subset of features) whereas bagging pulls from the same pool of features in each tree, making them essentially the same/correlated.}

	\item \textbf{(2 pts)} One useful application of bagging ensembles is to assess the relative importance of features. In your own words, describe how you think feature importance might be calculated when using Decision Trees as your base classifier. 

Now, using scikit-learnâ€™s examples as a guideline, generate a representation of the relative importance of each individual pixel for digit recognition (include this plot in your writeup). For this problem, you are allowed to use \verb+sklearn.ensemble.RandomForestClassifier+.

What do you observe? Is this surprising?

\textbf{Solution: \\ We can add up over all the trees how much each feature, when used to make a split, increases information gain. This can measure the importance of that feature by effectively measuring how much it contributes to information gain (how important it is in terms of gaining information about predicting the labels). \\
Below, we see a heat map of the feature importance for this problem. We notice there is just one single pixel in the center that is vastly important, while many pixels (on the periphery) are not that important. It makes sense that pixels on the periphery are not as important (they may not even contain the digit) while pixels towards the center are more important. It is a bit shocking however just how important that one pixel in the center is. 
\begin{center}
\includegraphics[width = 10cm, height = 10cm]{heatmap.png}
\end{center} }

\end{enumerate}

\end{document}